{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\vikram\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 'Female' ... 1 1 101348.88]\n",
      " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
      " [502 'France' 'Female' ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 'Female' ... 0 1 42085.58]\n",
      " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
      " [792 'France' 'Female' ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 1 ... 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "X[:, 2] = le.fit_transform(X[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[619 'France' 0 ... 1 1 101348.88]\n",
      " [608 'Spain' 0 ... 0 1 112542.58]\n",
      " [502 'France' 0 ... 1 0 113931.57]\n",
      " ...\n",
      " [709 'France' 0 ... 0 1 42085.58]\n",
      " [772 'Germany' 1 ... 1 0 92888.52]\n",
      " [792 'France' 0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [1])], remainder='passthrough')\n",
    "X = np.array(ct.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0 0.0 0.0 ... 1 1 101348.88]\n",
      " [0.0 0.0 1.0 ... 0 1 112542.58]\n",
      " [1.0 0.0 0.0 ... 1 0 113931.57]\n",
      " ...\n",
      " [1.0 0.0 0.0 ... 0 1 42085.58]\n",
      " [0.0 1.0 0.0 ... 1 0 92888.52]\n",
      " [1.0 0.0 0.0 ... 1 0 38190.78]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200/7200 [==============================] - 1s 101us/sample - loss: 0.5909 - acc: 0.7603\n",
      "7200/7200 [==============================] - 1s 124us/sample - loss: 0.5193 - acc: 0.7968\n",
      "7200/7200 [==============================] - 1s 97us/sample - loss: 0.6058 - acc: 0.7376\n",
      "7200/7200 [==============================] - 1s 104us/sample - loss: 0.5543 - acc: 0.7949\n",
      "7200/7200 [==============================] - 1s 102us/sample - loss: 0.6115 - acc: 0.7004\n",
      "7200/7200 [==============================] - 1s 101us/sample - loss: 0.5764 - acc: 0.7394\n",
      "7200/7200 [==============================] - 1s 104us/sample - loss: 0.5824 - acc: 0.7303\n",
      "7200/7200 [==============================] - 1s 103us/sample - loss: 0.5222 - acc: 0.7962\n",
      "7200/7200 [==============================] - 1s 114us/sample - loss: 0.6372 - acc: 0.6867\n",
      "7200/7200 [==============================] - 1s 107us/sample - loss: 0.5573 - acc: 0.7529\n",
      "7200/7200 [==============================] - 1s 111us/sample - loss: 0.6997 - acc: 0.5932\n",
      "7200/7200 [==============================] - 1s 111us/sample - loss: 0.5494 - acc: 0.7464\n",
      "7200/7200 [==============================] - 1s 116us/sample - loss: 0.5292 - acc: 0.7700\n",
      "7200/7200 [==============================] - 1s 117us/sample - loss: 0.7282 - acc: 0.5856\n",
      "7200/7200 [==============================] - 1s 118us/sample - loss: 0.5280 - acc: 0.7824\n",
      "7200/7200 [==============================] - 1s 116us/sample - loss: 0.5863 - acc: 0.7121\n",
      "7200/7200 [==============================] - 1s 121us/sample - loss: 0.5742 - acc: 0.7311\n",
      "7200/7200 [==============================] - 1s 129us/sample - loss: 0.5632 - acc: 0.7590\n",
      "7200/7200 [==============================] - 1s 128us/sample - loss: 0.5752 - acc: 0.7571\n",
      "7200/7200 [==============================] - 1s 125us/sample - loss: 0.5872 - acc: 0.7772\n",
      "7200/7200 [==============================] - 1s 127us/sample - loss: 0.5725 - acc: 0.7604\n",
      "7200/7200 [==============================] - 1s 128us/sample - loss: 0.5419 - acc: 0.7892\n",
      "7200/7200 [==============================] - 1s 128us/sample - loss: 0.5643 - acc: 0.7274\n",
      "7200/7200 [==============================] - 1s 128us/sample - loss: 0.5780 - acc: 0.7824\n",
      "7200/7200 [==============================] - 1s 131us/sample - loss: 0.6272 - acc: 0.7068\n",
      "7200/7200 [==============================] - 1s 138us/sample - loss: 0.5512 - acc: 0.7796\n",
      "7200/7200 [==============================] - 1s 139us/sample - loss: 0.4949 - acc: 0.7924\n",
      "7200/7200 [==============================] - 1s 139us/sample - loss: 0.5279 - acc: 0.7757\n",
      "7200/7200 [==============================] - 1s 160us/sample - loss: 0.5790 - acc: 0.7343\n",
      "7200/7200 [==============================] - 1s 144us/sample - loss: 0.6909 - acc: 0.5682\n",
      "7200/7200 [==============================] - 1s 176us/sample - loss: 0.5521 - acc: 0.7899\n",
      "7200/7200 [==============================] - 1s 176us/sample - loss: 0.5136 - acc: 0.7933\n",
      "7200/7200 [==============================] - 2s 288us/sample - loss: 0.5508 - acc: 0.7683\n",
      "7200/7200 [==============================] - 2s 287us/sample - loss: 0.6158 - acc: 0.7418\n",
      "7200/7200 [==============================] - 2s 325us/sample - loss: 0.6647 - acc: 0.6371\n",
      "7200/7200 [==============================] - 2s 209us/sample - loss: 0.5422 - acc: 0.7771\n",
      "7200/7200 [==============================] - 2s 211us/sample - loss: 0.5893 - acc: 0.7546\n",
      "7200/7200 [==============================] - 2s 223us/sample - loss: 0.5719 - acc: 0.7601\n",
      "7200/7200 [==============================] - 2s 210us/sample - loss: 0.5117 - acc: 0.7956\n",
      "7200/7200 [==============================] - 1s 150us/sample - loss: 0.5558 - acc: 0.7447\n",
      "7200/7200 [==============================] - 1s 142us/sample - loss: 0.5662 - acc: 0.7971\n",
      "7200/7200 [==============================] - 1s 132us/sample - loss: 0.5356 - acc: 0.7967\n",
      "7200/7200 [==============================] - 1s 134us/sample - loss: 0.5916 - acc: 0.7324\n",
      "7200/7200 [==============================] - 1s 134us/sample - loss: 0.5101 - acc: 0.7975\n",
      "7200/7200 [==============================] - 1s 141us/sample - loss: 0.5533 - acc: 0.7400\n",
      "7200/7200 [==============================] - 1s 140us/sample - loss: 0.6451 - acc: 0.6303\n",
      "7200/7200 [==============================] - 1s 142us/sample - loss: 0.5026 - acc: 0.7921\n",
      "7200/7200 [==============================] - 1s 138us/sample - loss: 0.5322 - acc: 0.7962\n",
      "7200/7200 [==============================] - 1s 146us/sample - loss: 0.6193 - acc: 0.6671\n",
      "7200/7200 [==============================] - 1s 174us/sample - loss: 0.5599 - acc: 0.7147\n",
      "7200/7200 [==============================] - 1s 172us/sample - loss: 0.6032 - acc: 0.7125\n",
      "7200/7200 [==============================] - 1s 154us/sample - loss: 0.6476 - acc: 0.6349\n",
      "7200/7200 [==============================] - 1s 205us/sample - loss: 0.6101 - acc: 0.6811\n",
      "7200/7200 [==============================] - 1s 155us/sample - loss: 0.5140 - acc: 0.7969\n",
      "7200/7200 [==============================] - 1s 150us/sample - loss: 0.6575 - acc: 0.6110\n",
      "7200/7200 [==============================] - 1s 164us/sample - loss: 0.5140 - acc: 0.7782\n",
      "7200/7200 [==============================] - 1s 160us/sample - loss: 0.5769 - acc: 0.7017\n",
      "7200/7200 [==============================] - 1s 179us/sample - loss: 0.5512 - acc: 0.7683\n",
      "7200/7200 [==============================] - 1s 167us/sample - loss: 0.5231 - acc: 0.7751\n",
      "7200/7200 [==============================] - 1s 165us/sample - loss: 0.5281 - acc: 0.7846\n",
      "7200/7200 [==============================] - 1s 172us/sample - loss: 0.5627 - acc: 0.7517\n",
      "7200/7200 [==============================] - 1s 191us/sample - loss: 0.5249 - acc: 0.7674\n",
      "7200/7200 [==============================] - 1s 185us/sample - loss: 0.6149 - acc: 0.6771\n",
      "7200/7200 [==============================] - 1s 198us/sample - loss: 0.6257 - acc: 0.7693\n",
      "7200/7200 [==============================] - 1s 171us/sample - loss: 0.7728 - acc: 0.5114\n",
      "7200/7200 [==============================] - 2s 338us/sample - loss: 0.5552 - acc: 0.7364\n",
      "7200/7200 [==============================] - 1s 178us/sample - loss: 0.5810 - acc: 0.7750\n",
      "7200/7200 [==============================] - 1s 181us/sample - loss: 0.5653 - acc: 0.7624\n",
      "7200/7200 [==============================] - 1s 169us/sample - loss: 0.5809 - acc: 0.7487\n",
      "7200/7200 [==============================] - 1s 177us/sample - loss: 0.6042 - acc: 0.6840\n",
      "7200/7200 [==============================] - 1s 188us/sample - loss: 0.5224 - acc: 0.7971\n",
      "7200/7200 [==============================] - 1s 189us/sample - loss: 0.6350 - acc: 0.7171\n",
      "7200/7200 [==============================] - 1s 196us/sample - loss: 0.5579 - acc: 0.7614\n",
      "7200/7200 [==============================] - 2s 254us/sample - loss: 0.5381 - acc: 0.7487\n",
      "7200/7200 [==============================] - 1s 178us/sample - loss: 0.6778 - acc: 0.6136\n",
      "7200/7200 [==============================] - 1s 185us/sample - loss: 0.6503 - acc: 0.6178\n",
      "7200/7200 [==============================] - 1s 182us/sample - loss: 0.6737 - acc: 0.6600\n",
      "7200/7200 [==============================] - 1s 186us/sample - loss: 0.5903 - acc: 0.7807\n",
      "7200/7200 [==============================] - 1s 185us/sample - loss: 0.6075 - acc: 0.7357\n",
      "7200/7200 [==============================] - 1s 183us/sample - loss: 0.5789 - acc: 0.7572\n",
      "8000/8000 [==============================] - 1s 176us/sample - loss: 0.5401 - acc: 0.7809\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def build_classifier(optimizer):\n",
    "    ann = tf.keras.models.Sequential()\n",
    "    ann.add(tf.keras.layers.Dense(units=6, activation='relu', input_dim = 12))\n",
    "    ann.add(tf.keras.layers.Dense(units=6, activation='relu'))\n",
    "    ann.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\n",
    "    ann.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return ann\n",
    "ann = tf.keras.wrappers.scikit_learn.KerasClassifier(build_fn = build_classifier)\n",
    "parameters = {'batch_size': [25,32], 'nb_epoch': [100,500], 'optimizer': ['adam', 'rmsprop']}\n",
    "accu = GridSearchCV(estimator = ann, param_grid = parameters, scoring ='accuracy', cv = 10)\n",
    "accu = accu.fit(X_train, y_train)\n",
    "bestp = accu.best_params_\n",
    "besta = accu.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32, 'nb_epoch': 100, 'optimizer': 'adam'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.799"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "besta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
